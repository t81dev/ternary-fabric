# Whitepaper: Accelerating AI via Ternary-Native Computing

**Abstract:**
Ternary Fabric is a high-performance co-processor architecture designed to leverage balanced ternary semantics ({-1, 0, 1}) for AI and signal processing. By eliminating traditional binary multipliers and replacing them with simple gating and sign-flipping logic, the architecture achieves extreme throughput and power efficiency. This paper describes the multi-tile scaling architecture, the PT-5 memory packing format, and the Zero-Skip optimization that exploits data sparsity.

## 1. Introduction
Traditional binary computing faces the "memory wall" and the "power wall" in AI workloads. Binary multiplications are energy-intensive and require significant silicon area. The Ternary Fabric proposes an alternative: executing directly on ternary-quantized weights and inputs.

## 2. Architecture
The fabric consists of multiple parallel tiles (default 4), each containing 15 Ternary Processing Element (TPE) lanes.
- **PT-5 Packing:** Packs 5 trits into 8 bits, achieving 95.1% storage efficiency and maximizing AXI bus utilization.
- **Zero-Skip Logic:** Automatically suppresses clock toggling when operands are zero, leading to significant dynamic power savings in sparse models.
- **Multi-Tile Scaling:** Supports weight broadcast and independent tile execution, scaling linearly with the number of tiles.

## 3. Performance Benchmarks

Experiments were conducted on the cycle-accurate fabric mock using varied matrix sizes and 50% sparsity.

| Configuration | Matrix Size | Cycles | Peak Throughput (GOPS) | Sparsity Exploited |
| :--- | :--- | :--- | :--- | :--- |
| **Single Tile** | 4x4 | 4 | 2.0 | 75% |
| **Multi-Tile (4)** | 16x15 | 16 | 30.0 | 65% |
| **Full Fabric (4)** | 100x15 | 100 | 30.0 | 66% |

### 3.1 Throughput Analysis
The fabric reaches its peak theoretical throughput of **30.0 GOPS** at 250 MHz for multi-tile configurations. This throughput is achieved with a minimal hardware footprint compared to binary equivalents.

### 3.2 Sparsity Advantage
With Zero-Skip enabled, the fabric avoids unnecessary computations. At 50% random sparsity, the architecture demonstrated an average of **65-70%** effective skip rate in lanes, directly translating to power efficiency.

## 4. Transparent Acceleration (Phase 9)
The project has successfully moved beyond core RTL to a full-stack **Transparent Acceleration** layer. By using memory interposition (LD_PRELOAD), the fabric can now accelerate existing applications like `llama.cpp` without source code modifications. This includes:
- **LRU Paging:** Efficiently managing model weights that exceed the 128MB Fabric pool.
- **Async Execution:** Overlapping CPU token sampling with Fabric GEMV offloading.
- **CPU Short-Circuiting:** Dynamically bypassing redundant CPU compute loops.

## 5. Future Directions & Experimental Kernels
With the core multi-tile architecture and transparent acceleration layer (Phase 9) now validated, the project is expanding into complex recurrent and attention-based structures. Current experimental kernels include **T-LSTM** and **T-Attention**, which are undergoing functional verification in the software reference layer before being committed to RTL.

## 6. Conclusion
The Ternary Fabric provides a scalable, efficient, and high-throughput substrate for the next generation of ternary-quantized neural networks. The successful implementation of Phase 9 demonstrates that ternary-native computing can scale and integrate seamlessly with modern AI frameworks, providing a fraction of the power and area overhead of binary systems.

---
Â© 2026 Ternary Fabric Project.
